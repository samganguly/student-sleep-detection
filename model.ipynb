{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6726a35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1234 images belonging to 2 classes.\n",
      "Found 218 images belonging to 2 classes.\n",
      "38 6\n",
      "WARNING:tensorflow:From C:\\Users\\Samrat\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Samrat\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Samrat\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samrat\\AppData\\Local\\Temp\\ipykernel_30288\\1439669672.py:54: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_batch, validation_data=valid_batch,epochs=15,steps_per_epoch=SPE ,validation_steps=VS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:From C:\\Users\\Samrat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Samrat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "38/38 [==============================] - 10s 234ms/step - loss: 0.4427 - accuracy: 0.7920 - val_loss: 0.1803 - val_accuracy: 0.9479\n",
      "Epoch 2/15\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.1973 - accuracy: 0.9285 - val_loss: 0.1189 - val_accuracy: 0.9583\n",
      "Epoch 3/15\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.1394 - accuracy: 0.9443 - val_loss: 0.1184 - val_accuracy: 0.9479\n",
      "Epoch 4/15\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.1093 - accuracy: 0.9559 - val_loss: 0.1080 - val_accuracy: 0.9583\n",
      "Epoch 5/15\n",
      "38/38 [==============================] - 2s 57ms/step - loss: 0.0887 - accuracy: 0.9667 - val_loss: 0.0947 - val_accuracy: 0.9479\n",
      "Epoch 6/15\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.1026 - accuracy: 0.9576 - val_loss: 0.0983 - val_accuracy: 0.9635\n",
      "Epoch 7/15\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.0558 - accuracy: 0.9775 - val_loss: 0.0960 - val_accuracy: 0.9740\n",
      "Epoch 8/15\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.0489 - accuracy: 0.9800 - val_loss: 0.0749 - val_accuracy: 0.9583\n",
      "Epoch 9/15\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0300 - accuracy: 0.9917 - val_loss: 0.0944 - val_accuracy: 0.9583\n",
      "Epoch 10/15\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0167 - accuracy: 0.9958 - val_loss: 0.1144 - val_accuracy: 0.9635\n",
      "Epoch 11/15\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0146 - accuracy: 0.9933 - val_loss: 0.0914 - val_accuracy: 0.9688\n",
      "Epoch 12/15\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0123 - accuracy: 0.9967 - val_loss: 0.0825 - val_accuracy: 0.9740\n",
      "Epoch 13/15\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.0273 - accuracy: 0.9917 - val_loss: 0.1044 - val_accuracy: 0.9688\n",
      "Epoch 14/15\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0215 - accuracy: 0.9933 - val_loss: 0.0622 - val_accuracy: 0.9740\n",
      "Epoch 15/15\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.0272 - accuracy: 0.9892 - val_loss: 0.1054 - val_accuracy: 0.9635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samrat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import random,shutil\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout,Conv2D,Flatten,Dense, MaxPooling2D, BatchNormalization\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "def generator(dir, gen=image.ImageDataGenerator(rescale=1./255), shuffle=True,batch_size=1,target_size=(24,24),class_mode='categorical' ):\n",
    "\n",
    "    return gen.flow_from_directory(dir,batch_size=batch_size,shuffle=shuffle,color_mode='grayscale',class_mode=class_mode,target_size=target_size)\n",
    "\n",
    "BS= 32\n",
    "TS=(24,24)\n",
    "train_batch= generator('data/train',shuffle=True, batch_size=BS,target_size=TS)\n",
    "valid_batch= generator('data/valid',shuffle=True, batch_size=BS,target_size=TS)\n",
    "SPE= len(train_batch.classes)//BS\n",
    "VS = len(valid_batch.classes)//BS\n",
    "print(SPE,VS)\n",
    "\n",
    "\n",
    "# img,labels= next(train_batch)\n",
    "# print(img.shape)\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(24,24,1)),\n",
    "    MaxPooling2D(pool_size=(1,1)),\n",
    "    Conv2D(32,(3,3),activation='relu'),\n",
    "    MaxPooling2D(pool_size=(1,1)),\n",
    "#32 convolution filters used each of size 3x3\n",
    "#again\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(1,1)),\n",
    "\n",
    "#64 convolution filters used each of size 3x3\n",
    "#choose the best features via pooling\n",
    "    \n",
    "#randomly turn neurons on and off to improve convergence\n",
    "    Dropout(0.25),\n",
    "#flatten since too many dimensions, we only want a classification output\n",
    "    Flatten(),\n",
    "#fully connected to get all relevant data\n",
    "    Dense(128, activation='relu'),\n",
    "#one more dropout for convergence' sake :) \n",
    "    Dropout(0.5),\n",
    "#output a softmax to squash the matrix into output probabilities\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(train_batch, validation_data=valid_batch,epochs=15,steps_per_epoch=SPE ,validation_steps=VS)\n",
    "\n",
    "model.save('models/cnnCat2.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcf9fe64",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m     15\u001b[0m image_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m24\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m train_data_generator \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_data_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategorical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m valid_data_generator \u001b[38;5;241m=\u001b[39m create_data_generator(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/valid\u001b[39m\u001b[38;5;124m'\u001b[39m, image_size, batch_size, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_data_generator\u001b[38;5;241m.\u001b[39mclasses) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m, in \u001b[0;36mcreate_data_generator\u001b[1;34m(directory, target_size, batch_size, class_mode)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_data_generator\u001b[39m(directory, target_size, batch_size, class_mode):\n\u001b[0;32m     11\u001b[0m     data_generator \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrayscale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\preprocessing\\image.py:1648\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[1;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m   1562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[0;32m   1563\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1564\u001b[0m     directory,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1578\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1579\u001b[0m ):\n\u001b[0;32m   1580\u001b[0m     \u001b[38;5;124;03m\"\"\"Takes the path to a directory & generates batches of augmented data.\u001b[39;00m\n\u001b[0;32m   1581\u001b[0m \n\u001b[0;32m   1582\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1646\u001b[0m \u001b[38;5;124;03m            and `y` is a numpy array of corresponding labels.\u001b[39;00m\n\u001b[0;32m   1647\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDirectoryIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1660\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\preprocessing\\image.py:563\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[1;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m    562\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    565\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data/train'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Conv2D, Flatten, Dense, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_data_generator(directory, target_size, batch_size, class_mode):\n",
    "    data_generator = ImageDataGenerator(rescale=1./255)\n",
    "    return data_generator.flow_from_directory(directory, target_size=target_size, batch_size=batch_size, color_mode='grayscale', class_mode=class_mode)\n",
    "\n",
    "batch_size = 32\n",
    "image_size = (24, 24)\n",
    "train_data_generator = create_data_generator('data/train', image_size, batch_size, 'categorical')\n",
    "valid_data_generator = create_data_generator('data/valid', image_size, batch_size, 'categorical')\n",
    "\n",
    "steps_per_epoch = len(train_data_generator.classes) // batch_size\n",
    "validation_steps = len(valid_data_generator.classes) // batch_size\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(24, 24, 1)),\n",
    "    MaxPooling2D(pool_size=(1, 1)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(1, 1)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(1, 1)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data_generator, validation_data=valid_data_generator, epochs=15, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps)\n",
    "\n",
    "model.save('models/cnn_cat_classification.h5', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7c36e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1234 images belonging to 2 classes.\n",
      "Found 218 images belonging to 2 classes.\n",
      "Epoch 1/15\n",
      "38/38 [==============================] - 4s 88ms/step - loss: 0.4311 - accuracy: 0.8037 - val_loss: 0.2426 - val_accuracy: 0.8958\n",
      "Epoch 2/15\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 0.1779 - accuracy: 0.9334 - val_loss: 0.1716 - val_accuracy: 0.9427\n",
      "Epoch 3/15\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 0.1453 - accuracy: 0.9443 - val_loss: 0.1385 - val_accuracy: 0.9479\n",
      "Epoch 4/15\n",
      "38/38 [==============================] - 3s 85ms/step - loss: 0.0897 - accuracy: 0.9700 - val_loss: 0.0920 - val_accuracy: 0.9635\n",
      "Epoch 5/15\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 0.0720 - accuracy: 0.9734 - val_loss: 0.0776 - val_accuracy: 0.9688\n",
      "Epoch 6/15\n",
      "38/38 [==============================] - 3s 85ms/step - loss: 0.0607 - accuracy: 0.9817 - val_loss: 0.0809 - val_accuracy: 0.9583\n",
      "Epoch 7/15\n",
      "38/38 [==============================] - 3s 85ms/step - loss: 0.0435 - accuracy: 0.9809 - val_loss: 0.0833 - val_accuracy: 0.9688\n",
      "Epoch 8/15\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 0.0370 - accuracy: 0.9859 - val_loss: 0.0621 - val_accuracy: 0.9688\n",
      "Epoch 9/15\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 0.0193 - accuracy: 0.9950 - val_loss: 0.0698 - val_accuracy: 0.9740\n",
      "Epoch 10/15\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 0.0278 - accuracy: 0.9875 - val_loss: 0.0711 - val_accuracy: 0.9688\n",
      "Epoch 11/15\n",
      "38/38 [==============================] - 3s 86ms/step - loss: 0.0431 - accuracy: 0.9842 - val_loss: 0.0715 - val_accuracy: 0.9635\n",
      "Epoch 12/15\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 0.0211 - accuracy: 0.9925 - val_loss: 0.0707 - val_accuracy: 0.9688\n",
      "Epoch 13/15\n",
      "38/38 [==============================] - 3s 86ms/step - loss: 0.0128 - accuracy: 0.9958 - val_loss: 0.0788 - val_accuracy: 0.9688\n",
      "Epoch 14/15\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 0.0121 - accuracy: 0.9933 - val_loss: 0.0947 - val_accuracy: 0.9792\n",
      "Epoch 15/15\n",
      "38/38 [==============================] - 3s 85ms/step - loss: 0.0233 - accuracy: 0.9933 - val_loss: 0.0940 - val_accuracy: 0.9740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samrat\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Conv2D, Flatten, Dense, MaxPooling2D\n",
    "\n",
    "def create_data_generator(directory, target_size, batch_size, class_mode):\n",
    "    data_generator = ImageDataGenerator(rescale=1./255)\n",
    "    return data_generator.flow_from_directory(directory, target_size=target_size, batch_size=batch_size, color_mode='grayscale', class_mode=class_mode)\n",
    "\n",
    "batch_size = 32\n",
    "image_size = (24, 24)\n",
    "\n",
    "# Specify your dataset directory\n",
    "train_data_generator = create_data_generator('C:/Users/Samrat/Desktop/Student Falling Sleep Detection/data/train', image_size, batch_size, 'categorical')\n",
    "valid_data_generator = create_data_generator('C:/Users/Samrat/Desktop/Student Falling Sleep Detection/data/valid', image_size, batch_size, 'categorical')\n",
    "\n",
    "\n",
    "steps_per_epoch = len(train_data_generator.classes) // batch_size\n",
    "validation_steps = len(valid_data_generator.classes) // batch_size\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(24, 24, 1)),\n",
    "    MaxPooling2D(pool_size=(1, 1)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(1, 1)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(1, 1)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data_generator, validation_data=valid_data_generator, epochs=15, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps)\n",
    "\n",
    "model.save('models/cnn_eye_classification.h5', overwrite=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
